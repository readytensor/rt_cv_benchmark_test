import os
import math
import torch
import numpy as np
from tqdm import tqdm
from typing import Tuple
from config import paths
from typing import Union, Dict, List
from torch.utils.data.dataloader import DataLoader
from torch.optim.lr_scheduler import LambdaLR
from score import evaluate_metrics
from torch.nn import CrossEntropyLoss, MultiMarginLoss
from torch.nn.functional import softmax
from torch_utils.early_stopping import EarlyStopping
from logger import get_logger

logger = get_logger(task_name="model")


os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"


class BaseTrainer:
    def __init__(
        self,
        model,
        train_loader,
        test_loader,
        validation_loader,
        num_samples=20,
        early_stopping: bool = True,
        early_stopping_patience: int = 10,
        early_stopping_delta: float = 0.05,
        loss_function=torch.nn.CrossEntropyLoss(),
        output_folder=paths.OUTPUTS_DIR,
    ):
        self.model = model
        self.output_folder = output_folder
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.validation_loader = validation_loader
        self.loss_function = loss_function
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.num_samples = num_samples
        self.early_stopping = early_stopping
        self.early_stopping_patience = early_stopping_patience
        self.early_stopping_delta = early_stopping_delta
        self.model.to(self.device)
        os.makedirs(self.output_folder, exist_ok=True)

        # Initialize class names from train loader if available
        if hasattr(train_loader.dataset, "classes"):
            self.class_names = train_loader.dataset.classes
        else:
            self.class_names = [str(i) for i in range(len(train_loader.dataset))]

        self.num_classes = len(self.class_names)

    def set_device(self, device: str) -> None:
        """
        Sets the device used for training.

        Args:
            device (str): Name of the device (ex: "cuda", "cpu").

        Returns: None
        """
        self.device = torch.device(device)
        self.model.to(self.device)

    def set_loss_function(
        self, loss_function: Union[CrossEntropyLoss, MultiMarginLoss]
    ) -> None:
        """
        Sets loss function used in training.

        Args:
            loss_function (Union[CrossEntropyLoss, MultiMarginLoss]): Loss function.

        Returns: None
        """
        self.loss_function = loss_function

    def create_metrics_history_dict(self, phase: str) -> Dict[str, List]:
        """
        Creates a dictionary to track the history of various metrics for a given phase.

        This method initializes a dictionary with lists to keep track of epoch numbers and
        various metrics such as loss, accuracy, macro recall, macro precision, macro F1 score,
        weighted recall, weighted precision, weighted F1 score, top-k accuracy, and mean
        average precision for the specified phase (e.g., training, validation, or testing).

        Parameters:
        - phase (str): The phase for which the metrics are being tracked. Typically, this
          would be 'train', 'validation', or 'test'.

        Returns:
        - Dict[str, List]: A dictionary with keys for each metric to be tracked, where each
          key maps to a list for recording the history of that metric over epochs.
        """

        metrics_history = {
            "epoch": [],
            f"{phase} loss": [],
            f"{phase} accuracy": [],
            f"{phase} macro_recall": [],
            f"{phase} macro_precision": [],
            f"{phase} macro_f1": [],
            f"{phase} weighted_recall": [],
            f"{phase} weighted_precision": [],
            f"{phase} weighted_f1": [],
            f"{phase} top_k_accuracy": [],
            f"{phase} mean_average_precision": [],
        }
        return metrics_history

    def update_metrics_history_dict(
        self,
        phase: str,
        metrics_history: dict[str, list],
        score_dict: dict,
        epoch_num: int = None,
    ) -> Dict[str, List]:
        if epoch_num:
            metrics_history["epoch"].append(epoch_num)
        for key in metrics_history.keys():
            if key.startswith(phase):
                score_key = key.removeprefix(f"{phase} ")
                metrics_history[key].append(score_dict[score_key])

        return metrics_history

    def train(
        self, num_epochs: int = 40, checkpoint_dir_path: str = paths.CHECKPOINTS_DIR
    ) -> Dict[str, List]:
        """
        Train the model on the data and calculate metrics per epoch on train and validation datasets.

        Args:
            num_epochs (int): The number of epochs.
            checkpoint_dir_path (str): Path for directory in which checkpoints are saved.


        Returns (Dict[str, List]): Train and validation metrics per epoch.
        example: {
            "Epoch": [0, 1, 2],
            "Train Loss": [1.5, 1.3, 1],
            "Validation Loss": [3, 2.5, 2.2],
        }
        """
        # Ensure at least 1 warmup epoch
        warmup_epochs = max(1, num_epochs // 5)
        self.model.train()

        scheduler = LambdaLR(
            self.optimizer,
            lr_lambda=self._warmup_cosine_annealing(0.001, warmup_epochs, num_epochs),
        )

        early_stopper = EarlyStopping(
            patience=self.early_stopping_patience,
            delta=self.early_stopping_delta,
            trace_func=logger.info,
        )

        total_batches = len(self.train_loader)
        metrics_history = self.create_metrics_history_dict(phase="train")

        if self.validation_loader:
            metrics_history.update(self.create_metrics_history_dict(phase="validation"))

        best_val_accuracy = 0.0  # Initialize best validation accuracy
        for epoch in range(num_epochs):
            logger.info(f"Starting epoch {epoch + 1}/{num_epochs}")
            running_loss = 0.0
            train_progress_bar = tqdm(
                total=total_batches, desc=f"Training - Epoch {epoch + 1}/{num_epochs}"
            )

            for inputs, labels in self.train_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                if isinstance(outputs, tuple):
                    outputs = outputs[0]
                probs = softmax(outputs, dim=1)
                _, predicted = torch.max(outputs.data, 1)
                loss = self.loss_function(outputs, labels)
                loss.backward()

                train_metrics = evaluate_metrics(
                    labels=labels.cpu().numpy(),
                    predictions=predicted.cpu().numpy(),
                    probabilities=probs.data.cpu().numpy(),
                    loss_function=self.loss_function,
                    top_k=[5],
                    class_names=self.class_names,
                )
                self.optimizer.step()
                running_loss += loss.item()
                train_progress_bar.update(1)

            metrics_history = self.update_metrics_history_dict(
                phase="train",
                metrics_history=metrics_history,
                score_dict=train_metrics,
                epoch_num=epoch + 1,
            )

            if self.validation_loader:
                val_labels, val_pred, val_prob = self.predict(self.validation_loader)
                val_metrics = evaluate_metrics(
                    val_labels,
                    val_pred,
                    val_prob,
                    self.loss_function,
                    top_k=[5],
                    n_classes=self.num_classes,
                )
                # Checkpointing
                if val_metrics["accuracy"] > best_val_accuracy:
                    best_val_accuracy = val_metrics["accuracy"]
                    self._save_checkpoint(epoch, checkpoint_dir_path)

                metrics_history = self.update_metrics_history_dict(
                    phase="validation",
                    metrics_history=metrics_history,
                    score_dict=val_metrics,
                )
                logger.info(f"Validation metrics after epoch {epoch}: {val_metrics}")

            logger.info(f"Training metrics after epoch {epoch}: {train_metrics}")
            scheduler.step()

            if self.early_stopping:
                loss = (
                    metrics_history["validation loss"][-1]
                    if self.validation_loader
                    else metrics_history["train loss"][-1]
                )
                if early_stopper(loss):
                    break

        train_progress_bar.close()

        return metrics_history

    def _save_checkpoint(self, epoch: int, output_folder: str) -> None:
        """
        Saves a checkpoint of the model.

        Args:
            epoch (int): The current epoch number.
            output_folder (str): The directory where the checkpoint will be saved.
        """
        checkpoint_path = os.path.join(
            output_folder, f"model_checkpoint_epoch_{epoch}.pth"
        )
        torch.save(self.model.state_dict(), checkpoint_path)
        logger.info(f"Checkpoint saved to {checkpoint_path}")

    def _warmup_cosine_annealing(self, base_lr, warmup_epochs, num_epochs):
        def lr_lambda(epoch):
            if epoch < warmup_epochs:
                return base_lr * (epoch / warmup_epochs)
            else:
                return base_lr * (
                    0.5
                    * (
                        1
                        + math.cos(
                            math.pi
                            * (epoch - warmup_epochs)
                            / (num_epochs - warmup_epochs)
                        )
                    )
                )

        return lr_lambda

    def predict(
        self, data_loader: DataLoader
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Predicts the class labels and logits for the data in a data loader.

        Args:
            data_loader (DataLoader): The input data.

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]: (Truth labels, Predicted class labels, Probabilities).
        """
        self.model.eval()
        self.model.to(self.device)
        with torch.no_grad():
            all_labels, all_predicted, all_probs = (
                np.array([]),
                np.array([]),
                np.array([]),
            )

            for inputs, labels in data_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                probs = softmax(outputs, dim=1)
                _, predicted = torch.max(outputs.data, 1)

                # Convert tensors to numpy arrays before appending
                all_predicted = np.append(all_predicted, predicted.cpu().numpy())
                all_labels = np.append(all_labels, labels.cpu().numpy())
                all_probs = (
                    np.concatenate((all_probs, probs.cpu().numpy()), axis=0)
                    if all_probs.size
                    else probs.cpu().numpy()
                )

        return all_labels, all_predicted, all_probs
